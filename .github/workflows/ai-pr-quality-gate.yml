name: AI PR Quality Gate

# AI-powered PR review using GitHub Copilot CLI

# Invokes security, qa, and analyst agents to review PRs IN PARALLEL

# Blocks merge if CRITICAL_FAIL detected

on:
  pull_request:
    branches: [main]
    types: [opened, synchronize, reopened]
    # NO paths filter - use dorny/paths-filter internally to satisfy required checks

  workflow_dispatch:
    inputs:
      pr_number:
        description: 'PR number to review'
        required: true
        type: number
      enable_debouncing:
        description: 'Enable debouncing delay to reduce race conditions (adds 10s latency)'
        required: false
        type: boolean
        default: false

permissions:
  contents: read
  pull-requests: write

# Concurrency control: Attempts to cancel in-progress runs when new runs start

# NOTE: GitHub Actions does NOT guarantee run coalescing - race conditions can occur

# where multiple runs start before cancellation takes effect. Multiple review runs

# may execute simultaneously, wasting compute until cancellation completes.

# See ADR-026 (../.agents/architecture/ADR-026-pr-automation-concurrency-and-safety.md)

# for architectural decision on workflow-level concurrency control

# Mitigation: Path filtering, timeouts, and PR-specific temp files reduce impact

concurrency:
  group: ai-quality-${{ github.event.pull_request.number || inputs.pr_number }}
  cancel-in-progress: true

env:

# Compute PR number from either pull_request event or workflow_dispatch input

  PR_NUMBER: ${{ github.event.pull_request.number || inputs.pr_number }}

jobs:
  # Optional debouncing job - runs only when enable_debouncing=true
  debounce:
    name: Debounce Workflow
    if: (github.event_name == 'workflow_dispatch' && inputs.enable_debouncing == true) || (github.event_name == 'pull_request' && vars.ENABLE_DEBOUNCE == 'true')
    runs-on: ubuntu-24.04-arm
    steps:
      - name: Checkout repository
        uses: actions/checkout@34e114876b0b11c390a56381ad16ebd13914f8d5
      
      - name: Apply debouncing delay
        uses: ./.github/actions/workflow-debounce
        with:
          delay-seconds: '10'
          workflow-name: 'AI PR Quality Gate'
          concurrency-group: ${{ github.event.pull_request.number || inputs.pr_number }}

# Check if relevant files changed using paths-filter

  check-changes:
    name: Check Changes
    # ADR-025: ARM runner for cost optimization (37.5% savings vs x64)
    runs-on: ubuntu-24.04-arm
    needs: debounce
    if: |-
      always() &&
      (needs.debounce.result == 'success' || needs.debounce.result == 'skipped') &&
      github.actor != 'dependabot[bot]' && github.actor != 'github-actions[bot]'

    outputs:
      # For workflow_dispatch, always run review (manual trigger assumes user intent regardless of files changed); otherwise use paths-filter result
      should-run: ${{ github.event_name == 'workflow_dispatch' && 'true' || steps.determine.outputs.should-run }}

    steps:
      - name: Checkout repository
        uses: actions/checkout@34e114876b0b11c390a56381ad16ebd13914f8d5

      - name: Check for relevant file changes
        uses: dorny/paths-filter@de90cc6fb38fc0963ad72b210f1f284cd68cea36 # v3
        id: filter
        # Skip paths-filter for workflow_dispatch (path filter requires pull_request event context; manual dispatch lacks base/head ref information)
        if: github.event_name != 'workflow_dispatch'
        with:
          filters: |
            relevant:
              - 'src/**'
              - 'scripts/**'
              - 'build/**'
              - '.github/**'
              - '.agents/**'
              - '.claude/skills/**'

      - name: Determine if review should run
        id: determine
        run: |
          if [ "${{ github.event_name }}" = "workflow_dispatch" ]; then
            echo "should-run=true" >> $GITHUB_OUTPUT
            echo "Decision: Run review (manual trigger)"
          else
            RELEVANT="${{ steps.filter.outputs.relevant }}"

            echo "Relevant files changed: $RELEVANT"

            # Run review if relevant files changed, skip otherwise
            if [ "$RELEVANT" = "true" ]; then
              echo "should-run=true" >> $GITHUB_OUTPUT
              echo "Decision: Run review (relevant files changed)"
            else
              echo "should-run=false" >> $GITHUB_OUTPUT
              echo "Decision: Skip review (no relevant files changed)"
            fi
          fi

# ==============================================================================
# Agent Review Jobs (6 Static Jobs)
# ==============================================================================
#
# Each agent (security, qa, analyst, architect, devops, roadmap) has a dedicated
# job with a static name. This ensures status check contexts are always emitted
# with proper names, even when reviews are skipped due to file filtering.
#
# Why not matrix? Job-level `if:` conditions prevent matrix expansion, leaving
# check context names as unresolved placeholders (e.g., "${{ matrix.agent }} Review").
# Static jobs with step-level conditionals solve this by:
# 1. Job names resolve at parse time (not runtime)
# 2. Jobs always run, emitting proper status checks
# 3. Composite action (.github/actions/agent-review) encapsulates common workflow
#
# Trade-off: Six job definitions (CVA) vs correctness (reliable status checks).

  security-review:
    name: Security Review
    runs-on: ubuntu-24.04-arm
    needs: check-changes
    if: always() && needs.check-changes.result == 'success'
    timeout-minutes: 10
    permissions:
      contents: read
      pull-requests: write

    steps:
      - name: Run security review
        uses: ./.github/actions/agent-review
        with:
          agent: security
          emoji: üîí
          prompt-file: .github/prompts/pr-quality-gate-security.md
          should-run: ${{ needs.check-changes.outputs.should-run }}
          pr-number: ${{ env.PR_NUMBER }}
          bot-pat: ${{ secrets.BOT_PAT }}
          copilot-token: ${{ secrets.COPILOT_GITHUB_TOKEN }}

  qa-review:
    name: QA Review
    runs-on: ubuntu-24.04-arm
    needs: check-changes
    if: always() && needs.check-changes.result == 'success'
    timeout-minutes: 10
    permissions:
      contents: read
      pull-requests: write

    steps:
      - name: Checkout repository
        uses: actions/checkout@34e114876b0b11c390a56381ad16ebd13914f8d5

      - name: Run qa review
        uses: ./.github/actions/agent-review
        with:
          agent: qa
          emoji: üß™
          prompt-file: .github/prompts/pr-quality-gate-qa.md
          should-run: ${{ needs.check-changes.outputs.should-run }}
          pr-number: ${{ env.PR_NUMBER }}
          bot-pat: ${{ secrets.BOT_PAT }}
          copilot-token: ${{ secrets.COPILOT_GITHUB_TOKEN }}

  analyst-review:
    name: Analyst Review
    runs-on: ubuntu-24.04-arm
    needs: check-changes
    if: always() && needs.check-changes.result == 'success'
    timeout-minutes: 10
    permissions:
      contents: read
      pull-requests: write

    steps:
      - name: Checkout repository
        uses: actions/checkout@34e114876b0b11c390a56381ad16ebd13914f8d5

      - name: Run analyst review
        uses: ./.github/actions/agent-review
        with:
          agent: analyst
          emoji: üìä
          prompt-file: .github/prompts/pr-quality-gate-analyst.md
          should-run: ${{ needs.check-changes.outputs.should-run }}
          pr-number: ${{ env.PR_NUMBER }}
          bot-pat: ${{ secrets.BOT_PAT }}
          copilot-token: ${{ secrets.COPILOT_GITHUB_TOKEN }}

  architect-review:
    name: Architect Review
    runs-on: ubuntu-24.04-arm
    needs: check-changes
    if: always() && needs.check-changes.result == 'success'
    timeout-minutes: 10
    permissions:
      contents: read
      pull-requests: write

    steps:
      - name: Checkout repository
        uses: actions/checkout@34e114876b0b11c390a56381ad16ebd13914f8d5

      - name: Run architect review
        uses: ./.github/actions/agent-review
        with:
          agent: architect
          emoji: üìê
          prompt-file: .github/prompts/pr-quality-gate-architect.md
          should-run: ${{ needs.check-changes.outputs.should-run }}
          pr-number: ${{ env.PR_NUMBER }}
          bot-pat: ${{ secrets.BOT_PAT }}
          copilot-token: ${{ secrets.COPILOT_GITHUB_TOKEN }}

  devops-review:
    name: DevOps Review
    runs-on: ubuntu-24.04-arm
    needs: check-changes
    if: always() && needs.check-changes.result == 'success'
    timeout-minutes: 10
    permissions:
      contents: read
      pull-requests: write

    steps:
      - name: Checkout repository
        uses: actions/checkout@34e114876b0b11c390a56381ad16ebd13914f8d5

      - name: Run devops review
        uses: ./.github/actions/agent-review
        with:
          agent: devops
          emoji: ‚öôÔ∏è
          prompt-file: .github/prompts/pr-quality-gate-devops.md
          should-run: ${{ needs.check-changes.outputs.should-run }}
          pr-number: ${{ env.PR_NUMBER }}
          bot-pat: ${{ secrets.BOT_PAT }}
          copilot-token: ${{ secrets.COPILOT_GITHUB_TOKEN }}

  roadmap-review:
    name: Roadmap Review
    runs-on: ubuntu-24.04-arm
    needs: check-changes
    if: always() && needs.check-changes.result == 'success'
    timeout-minutes: 10
    permissions:
      contents: read
      pull-requests: write

    steps:
      - name: Checkout repository
        uses: actions/checkout@34e114876b0b11c390a56381ad16ebd13914f8d5

      - name: Run roadmap review
        uses: ./.github/actions/agent-review
        with:
          agent: roadmap
          emoji: üó∫Ô∏è
          prompt-file: .github/prompts/pr-quality-gate-roadmap.md
          should-run: ${{ needs.check-changes.outputs.should-run }}
          pr-number: ${{ env.PR_NUMBER }}
          bot-pat: ${{ secrets.BOT_PAT }}
          copilot-token: ${{ secrets.COPILOT_GITHUB_TOKEN }}

# ==============================================================================
# Aggregate Results from All Agent Reviews
# ==============================================================================
#
# Depends on all six agent review jobs explicitly (not matrix).
# Downloads artifacts using pattern `review-*` to gather all agent verdicts.

  aggregate:
    name: Aggregate Results
    # ADR-025: ARM runner for cost optimization (37.5% savings vs x64)
    runs-on: ubuntu-24.04-arm
    needs: [check-changes, security-review, qa-review, analyst-review, architect-review, devops-review, roadmap-review]
    # Always run if check-changes ran and said to run, even if some agent jobs failed
    if: always() && needs.check-changes.outputs.should-run == 'true'

    outputs:
      final-verdict: ${{ steps.aggregate.outputs.final_verdict }}

    steps:
      - name: Check for failed agents
        id: failed-agents
        shell: pwsh -NoProfile -Command "& '{0}'"
        run: |
          # Check if any agent review jobs failed by examining the needs context
          # This step runs before downloading artifacts to surface failures early
          # Note: Specific failed agents are identified later by examining individual verdicts (lines 673-682 in 'Check for Critical Failures' step)
          $agentResults = @(
            @{ Name = 'Security'; Result = "${{ needs.security-review.result }}" }
            @{ Name = 'QA'; Result = "${{ needs.qa-review.result }}" }
            @{ Name = 'Analyst'; Result = "${{ needs.analyst-review.result }}" }
            @{ Name = 'Architect'; Result = "${{ needs.architect-review.result }}" }
            @{ Name = 'DevOps'; Result = "${{ needs.devops-review.result }}" }
            @{ Name = 'Roadmap'; Result = "${{ needs.roadmap-review.result }}" }
          )

          Write-Output "Individual review results:"
          $failedAgents = @()

          foreach ($agent in $agentResults) {
            Write-Output "  $($agent.Name): $($agent.Result)"
            if ($agent.Result -in @('failure', 'cancelled')) {
              $failedAgents += $agent.Name
              Write-Output "::error::$($agent.Name) agent failed with result: $($agent.Result)"
            }
          }

          if ($failedAgents.Count -gt 0) {
            "has_failures=true" >> $env:GITHUB_OUTPUT
            Write-Output "::warning::Failed agents: $($failedAgents -join ', ')"
            Write-Output "::warning::Check individual job logs for details"
          } else {
            "has_failures=false" >> $env:GITHUB_OUTPUT
          }

      - name: Checkout repository
        uses: actions/checkout@34e114876b0b11c390a56381ad16ebd13914f8d5

      - name: Download all review artifacts
        uses: actions/download-artifact@d3f86a106a0bac45b974a628896c90dbdf5c8093
        with:
          pattern: review-*
          path: ai-review-results
          merge-multiple: true

      - name: Validate artifact download
        shell: pwsh -NoProfile -Command "& '{0}'"
        run: |
          Write-Output "Checking for required verdict files..."

          $agents = @('security', 'qa', 'analyst', 'architect', 'devops', 'roadmap')
          $missingFiles = @()

          foreach ($agent in $agents) {
            $verdictFile = "ai-review-results/$agent-verdict.txt"
            if (-not (Test-Path $verdictFile)) {
              $missingFiles += "$agent-verdict.txt"
            }
          }

          if ($missingFiles.Count -gt 0) {
            Write-Output "::error::Artifact download incomplete - missing files:"
            foreach ($file in $missingFiles) {
              Write-Output "::error::  - $file"
            }
            Write-Output "::error::This indicates agent review jobs failed or artifacts were not uploaded"
            exit 1
          }

          Write-Output "‚úì All required verdict files present"

      - name: Load review results
        id: load-results
        shell: pwsh -NoProfile -Command "& '{0}'"
        run: |
          $agents = @('security', 'qa', 'analyst', 'architect', 'devops', 'roadmap')

          Write-Output "Loaded verdicts:"

          foreach ($agent in $agents) {
            $verdictFile = "ai-review-results/$agent-verdict.txt"
            $infraFile = "ai-review-results/$agent-infrastructure-failure.txt"

            # Read verdict (file existence already validated)
            $verdict = (Get-Content $verdictFile -Raw).Trim()

            # Read infrastructure flag
            $infra = if (Test-Path $infraFile) { (Get-Content $infraFile -Raw).Trim() } else { "false" }

            Write-Output "  ${agent}: $verdict (infra: $infra)"

            # Export to GITHUB_OUTPUT
            "${agent}_verdict=$verdict" >> $env:GITHUB_OUTPUT
            "${agent}_infra=$infra" >> $env:GITHUB_OUTPUT
          }

      - name: Aggregate Verdicts
        id: aggregate
        shell: pwsh -NoProfile -Command "& '{0}'"
        run: |
          Import-Module "$env:GITHUB_WORKSPACE/.github/scripts/AIReviewCommon.psm1" -Force

          # Get verdicts from loaded results
          $securityVerdict = "${{ steps.load-results.outputs.security_verdict }}"
          $qaVerdict = "${{ steps.load-results.outputs.qa_verdict }}"
          $analystVerdict = "${{ steps.load-results.outputs.analyst_verdict }}"
          $architectVerdict = "${{ steps.load-results.outputs.architect_verdict }}"
          $devopsVerdict = "${{ steps.load-results.outputs.devops_verdict }}"
          $roadmapVerdict = "${{ steps.load-results.outputs.roadmap_verdict }}"

          # Get infrastructure failure flags
          $securityInfra = "${{ steps.load-results.outputs.security_infra }}" -eq "true"
          $qaInfra = "${{ steps.load-results.outputs.qa_infra }}" -eq "true"
          $analystInfra = "${{ steps.load-results.outputs.analyst_infra }}" -eq "true"
          $architectInfra = "${{ steps.load-results.outputs.architect_infra }}" -eq "true"
          $devopsInfra = "${{ steps.load-results.outputs.devops_infra }}" -eq "true"
          $roadmapInfra = "${{ steps.load-results.outputs.roadmap_infra }}" -eq "true"

          Write-Log "Security verdict: $securityVerdict (infra: $securityInfra)"
          Write-Log "QA verdict: $qaVerdict (infra: $qaInfra)"
          Write-Log "Analyst verdict: $analystVerdict (infra: $analystInfra)"
          Write-Log "Architect verdict: $architectVerdict (infra: $architectInfra)"
          Write-Log "DevOps verdict: $devopsVerdict (infra: $devopsInfra)"
          Write-Log "Roadmap verdict: $roadmapVerdict (infra: $roadmapInfra)"

          # Categorize failures: if verdict is failure AND infrastructure flag is true, it's INFRASTRUCTURE; otherwise, it's CODE_QUALITY
          # Infrastructure failures are downgraded to WARN (line 443) to prevent blocking PRs for transient service issues
          function Get-Category {
            param([string]$Verdict, [bool]$InfraFlag)
            if ($Verdict -in 'CRITICAL_FAIL', 'REJECTED', 'FAIL', 'NEEDS_REVIEW') {
              if ($InfraFlag) { return 'INFRASTRUCTURE' } else { return 'CODE_QUALITY' }
            }
            return 'N/A'
          }

          $securityCategory = Get-Category -Verdict $securityVerdict -InfraFlag $securityInfra
          $qaCategory = Get-Category -Verdict $qaVerdict -InfraFlag $qaInfra
          $analystCategory = Get-Category -Verdict $analystVerdict -InfraFlag $analystInfra
          $architectCategory = Get-Category -Verdict $architectVerdict -InfraFlag $architectInfra
          $devopsCategory = Get-Category -Verdict $devopsVerdict -InfraFlag $devopsInfra
          $roadmapCategory = Get-Category -Verdict $roadmapVerdict -InfraFlag $roadmapInfra

          Write-Log "Security category: $securityCategory"
          Write-Log "QA category: $qaCategory"
          Write-Log "Analyst category: $analystCategory"
          Write-Log "Architect category: $architectCategory"
          Write-Log "DevOps category: $devopsCategory"
          Write-Log "Roadmap category: $roadmapCategory"

          # Determine if any CODE_QUALITY failures exist
          $codeQualityFailures = @($securityCategory, $qaCategory, $analystCategory, $architectCategory, $devopsCategory, $roadmapCategory) -contains 'CODE_QUALITY'

          # Merge all verdicts
          $final = Merge-Verdicts -Verdicts @($securityVerdict, $qaVerdict, $analystVerdict, $architectVerdict, $devopsVerdict, $roadmapVerdict)
          Write-Log "Final verdict: $final"

          # If final verdict is failure but no CODE_QUALITY failures, change to WARN
          if ($final -in 'CRITICAL_FAIL', 'REJECTED', 'FAIL', 'NEEDS_REVIEW') {
            if (-not $codeQualityFailures) {
              Write-Log "All failures are INFRASTRUCTURE - downgrading to WARN (PR will not be blocked)"
              $final = 'WARN'
            }
          }

          "final_verdict=$final" >> $env:GITHUB_OUTPUT
          "security_verdict=$securityVerdict" >> $env:GITHUB_OUTPUT
          "qa_verdict=$qaVerdict" >> $env:GITHUB_OUTPUT
          "analyst_verdict=$analystVerdict" >> $env:GITHUB_OUTPUT
          "architect_verdict=$architectVerdict" >> $env:GITHUB_OUTPUT
          "devops_verdict=$devopsVerdict" >> $env:GITHUB_OUTPUT
          "roadmap_verdict=$roadmapVerdict" >> $env:GITHUB_OUTPUT

          # Output categories for PR comment
          "security_category=$securityCategory" >> $env:GITHUB_OUTPUT
          "qa_category=$qaCategory" >> $env:GITHUB_OUTPUT
          "analyst_category=$analystCategory" >> $env:GITHUB_OUTPUT
          "architect_category=$architectCategory" >> $env:GITHUB_OUTPUT
          "devops_category=$devopsCategory" >> $env:GITHUB_OUTPUT
          "roadmap_category=$roadmapCategory" >> $env:GITHUB_OUTPUT

      - name: Generate Report
        id: report
        shell: pwsh -NoProfile -Command "& '{0}'"
        env:
          RUN_ID: ${{ github.run_id }}
          SERVER_URL: ${{ github.server_url }}
          REPOSITORY: ${{ github.repository }}
          EVENT_NAME: ${{ github.event_name }}
          REF_NAME: ${{ github.ref_name }}
          SHA: ${{ github.sha }}
        run: |
          $ErrorActionPreference = 'Stop'

          try {
            Import-Module "$env:GITHUB_WORKSPACE/.github/scripts/AIReviewCommon.psm1" -Force -ErrorAction Stop

            $reportDir = Initialize-AIReview
            if (-not (Test-Path $reportDir)) {
              Write-Output "::error::Failed to initialize AI review directory: $reportDir"
              exit 1
            }

            $reportFile = Join-Path $reportDir "pr-quality-report.md"
          $finalVerdict = "${{ steps.aggregate.outputs.final_verdict }}"
          $alertType = Get-VerdictAlertType -Verdict $finalVerdict

          # Get verdicts
          $securityVerdict = "${{ steps.aggregate.outputs.security_verdict }}"
          $qaVerdict = "${{ steps.aggregate.outputs.qa_verdict }}"
          $analystVerdict = "${{ steps.aggregate.outputs.analyst_verdict }}"
          $architectVerdict = "${{ steps.aggregate.outputs.architect_verdict }}"
          $devopsVerdict = "${{ steps.aggregate.outputs.devops_verdict }}"
          $roadmapVerdict = "${{ steps.aggregate.outputs.roadmap_verdict }}"

          # Get categories
          $securityCategory = "${{ steps.aggregate.outputs.security_category }}"
          $qaCategory = "${{ steps.aggregate.outputs.qa_category }}"
          $analystCategory = "${{ steps.aggregate.outputs.analyst_category }}"
          $architectCategory = "${{ steps.aggregate.outputs.architect_category }}"
          $devopsCategory = "${{ steps.aggregate.outputs.devops_category }}"
          $roadmapCategory = "${{ steps.aggregate.outputs.roadmap_category }}"

          # Get emojis
          $securityEmoji = Get-VerdictEmoji -Verdict $securityVerdict
          $qaEmoji = Get-VerdictEmoji -Verdict $qaVerdict
          $analystEmoji = Get-VerdictEmoji -Verdict $analystVerdict
          $architectEmoji = Get-VerdictEmoji -Verdict $architectVerdict
          $devopsEmoji = Get-VerdictEmoji -Verdict $devopsVerdict
          $roadmapEmoji = Get-VerdictEmoji -Verdict $roadmapVerdict
          $finalEmoji = Get-VerdictEmoji -Verdict $finalVerdict

          # Build report content
          $report = @"
          <!-- AI-PR-QUALITY-GATE -->

          ## AI Quality Gate Review

          > [!$alertType]
          > $finalEmoji **Final Verdict: $finalVerdict**

          <details>
          <summary>Walkthrough</summary>

          This PR was reviewed by six AI agents **in parallel**, analyzing different aspects of the changes:

          - **Security Agent**: Scans for vulnerabilities, secrets exposure, and security anti-patterns
          - **QA Agent**: Evaluates test coverage, error handling, and code quality
          - **Analyst Agent**: Assesses code quality, impact analysis, and maintainability
          - **Architect Agent**: Reviews design patterns, system boundaries, and architectural concerns
          - **DevOps Agent**: Evaluates CI/CD, build pipelines, and infrastructure changes
          - **Roadmap Agent**: Assesses strategic alignment, feature scope, and user value

          </details>

          ### Review Summary

          | Agent | Verdict | Category | Status |
          |:------|:--------|:---------|:------:|
          | Security | $securityVerdict | $securityCategory | $securityEmoji |
          | QA | $qaVerdict | $qaCategory | $qaEmoji |
          | Analyst | $analystVerdict | $analystCategory | $analystEmoji |
          | Architect | $architectVerdict | $architectCategory | $architectEmoji |
          | DevOps | $devopsVerdict | $devopsCategory | $devopsEmoji |
          | Roadmap | $roadmapVerdict | $roadmapCategory | $roadmapEmoji |

          üí° **Quick Access**: Click on individual agent jobs (e.g., "üîí security Review", "üß™ qa Review") in the [workflow run]($env:SERVER_URL/$env:REPOSITORY/actions/runs/$env:RUN_ID) to see detailed findings and step summaries.

          "@

          # Add findings sections
          $findingsMap = @{
            'security' = 'Security Review Details'
            'qa' = 'QA Review Details'
            'analyst' = 'Analyst Review Details'
            'architect' = 'Architect Review Details'
            'devops' = 'DevOps Review Details'
            'roadmap' = 'Roadmap Review Details'
          }

            foreach ($agent in $findingsMap.Keys) {
              $findingsFile = "ai-review-results/$agent-findings.txt"
              if (Test-Path $findingsFile) {
                try {
                  $findings = Get-Content $findingsFile -Raw -ErrorAction Stop
                  if ($findings) {
                    $title = $findingsMap[$agent]
                    $report += "`n<details>`n<summary>$title</summary>`n`n$findings`n`n</details>`n"
                  } else {
                    Write-Warning "Findings file for $agent is empty"
                    $report += "`n<details>`n<summary>$($findingsMap[$agent])</summary>`n`n‚ö†Ô∏è No findings available (empty file)`n`n</details>`n"
                  }
                }
                catch {
                  Write-Output "::error::Failed to read findings file for ${agent}: $_"
                  $report += "`n<details>`n<summary>$($findingsMap[$agent])</summary>`n`n‚ùå Error reading findings file: $_`n`n</details>`n"
                }
              } else {
                Write-Warning "No $agent findings file found"
                $report += "`n<details>`n<summary>$($findingsMap[$agent])</summary>`n`n‚ö†Ô∏è Findings file not found (agent review may have failed)`n`n</details>`n"
              }
            }

            # Add footer
            $report += @"

            ---

            <details>
            <summary>Run Details</summary>

            | Property | Value |
            |:---------|:------|
            | **Run ID** | [$env:RUN_ID]($env:SERVER_URL/$env:REPOSITORY/actions/runs/$env:RUN_ID) |
            | **Triggered by** | ``$env:EVENT_NAME`` on ``$env:REF_NAME`` |
            | **Commit** | ``$env:SHA`` |

            </details>

            <sub>Powered by [AI Quality Gate](https://github.com/$env:REPOSITORY) workflow</sub>
            "@

            $report | Set-Content $reportFile -Encoding UTF8 -ErrorAction Stop

            if (-not (Test-Path $reportFile)) {
              Write-Output "::error::Failed to write report file: $reportFile"
              exit 1
            }

            "report_file=$reportFile" >> $env:GITHUB_OUTPUT
          }
          catch {
            Write-Output "::error::Failed to generate report: $_"
            Write-Output "::error::Module: AIReviewCommon.psm1"
            exit 1
          }

      - name: Check for infrastructure failures and add label
        shell: pwsh -NoProfile -Command "& '{0}'"
        env:
          GH_TOKEN: ${{ github.token }}
          PR_NUMBER: ${{ env.PR_NUMBER }}
        run: |
          $ErrorActionPreference = 'Stop'

          # Check if any agent had infrastructure failure (Issue #328: Add infrastructure-failure label when retry logic exhausted)
          $infrastructureFailureDetected = $false
          $retryAttempts = @()

          $agents = @('security', 'qa', 'analyst', 'architect', 'devops', 'roadmap')

          foreach ($agent in $agents) {
            $infraFile = "ai-review-results/$agent-infrastructure-failure.txt"
            $retryFile = "ai-review-results/$agent-retry-count.txt"

            if (Test-Path $infraFile) {
              $infraFlag = (Get-Content $infraFile -Raw).Trim()
              if ($infraFlag -eq "true") {
                $infrastructureFailureDetected = $true
                $retryCount = 0
                if (Test-Path $retryFile) {
                  $retryCount = [int](Get-Content $retryFile -Raw).Trim()
                }
                $retryAttempts += "${agent}: $retryCount retries"
                Write-Output "::notice::Infrastructure failure detected for $agent agent (retries: $retryCount)"
              }
            }
          }

          if ($infrastructureFailureDetected) {
            Write-Output "::warning::Infrastructure failures detected - adding infrastructure-failure label"
            Write-Output "Retry attempts: $($retryAttempts -join ', ')"

            try {
              # Verify gh CLI is authenticated
              $authStatus = gh auth status 2>&1
              if ($LASTEXITCODE -ne 0) {
                Write-Output "::error::gh CLI authentication failed: $authStatus"
                Write-Output "::error::Cannot add infrastructure-failure label"
                exit 1
              }

              # Add label with error handling
              gh pr edit $env:PR_NUMBER --repo $env:GITHUB_REPOSITORY --add-label "infrastructure-failure"

              if ($LASTEXITCODE -ne 0) {
                Write-Output "::error::Failed to add infrastructure-failure label (exit code: $LASTEXITCODE)"
                Write-Output "::error::Ensure 'infrastructure-failure' label exists in repository"
                exit 1
              }

              Write-Output "‚úì Successfully added infrastructure-failure label"
            }
            catch {
              Write-Output "::error::Failed to add label: $_"
              exit 1
            }
          } else {
            Write-Output "No infrastructure failures detected"
          }

      - name: Post PR Comment
        shell: pwsh -NoProfile -Command "& '{0}'"
        env:
          GH_TOKEN: ${{ github.token }}
          PR_NUMBER: ${{ env.PR_NUMBER }}
          REPORT_FILE: ${{ steps.report.outputs.report_file }}
        run: |
          $ErrorActionPreference = 'Stop'

          if ([string]::IsNullOrWhiteSpace($env:PR_NUMBER)) {
            Write-Output "::error::PR_NUMBER environment variable is missing"
            exit 1
          }

          if ([string]::IsNullOrWhiteSpace($env:REPORT_FILE)) {
            Write-Output "::error::REPORT_FILE environment variable is missing"
            exit 1
          }

          if (-not (Test-Path $env:REPORT_FILE)) {
            Write-Output "::error::Report file not found: $env:REPORT_FILE"
            exit 1
          }

          $skillScript = ".claude/skills/github/scripts/issue/Post-IssueComment.ps1"
          if (-not (Test-Path $skillScript)) {
            Write-Output "::error::Skill script not found: $skillScript"
            exit 1
          }

          try {
            # Use GitHub skill script for idempotent comment posting
            # PRs are issues in GitHub API (pull requests are a superset of issues), so we use Post-IssueComment with marker `AI-PR-QUALITY-GATE`
            # UpdateIfExists updates the existing marked comment if present, ensuring the latest report is reflected
            & $skillScript `
              -Issue $env:PR_NUMBER `
              -BodyFile $env:REPORT_FILE `
              -Marker "AI-PR-QUALITY-GATE" `
              -UpdateIfExists

            if ($LASTEXITCODE -ne 0) {
              Write-Output "::error::Post-IssueComment.ps1 failed with exit code $LASTEXITCODE"
              exit $LASTEXITCODE
            }
          }
          catch {
            Write-Output "::error::Failed to post PR comment: $_"
            exit 1
          }

      - name: Set Job Summary
        shell: pwsh -NoProfile -Command "& '{0}'"
        env:
          REPORT_FILE: ${{ steps.report.outputs.report_file }}
        run: |
          Get-Content $env:REPORT_FILE | Add-Content $env:GITHUB_STEP_SUMMARY

      - name: Check for Critical Failures
        shell: pwsh -NoProfile -Command "& '{0}'"
        run: |
          $finalVerdict = "${{ steps.aggregate.outputs.final_verdict }}"

          # Use ordered array for deterministic output (PowerShell dictionary Keys property is unordered, which would cause agent order to vary across runs, complicating comparisons)
          $agentVerdicts = @(
            @{ Name = 'üîí Security'; Verdict = "${{ steps.aggregate.outputs.security_verdict }}" }
            @{ Name = 'üß™ QA'; Verdict = "${{ steps.aggregate.outputs.qa_verdict }}" }
            @{ Name = 'üìä Analyst'; Verdict = "${{ steps.aggregate.outputs.analyst_verdict }}" }
            @{ Name = 'üìê Architect'; Verdict = "${{ steps.aggregate.outputs.architect_verdict }}" }
            @{ Name = '‚öôÔ∏è DevOps'; Verdict = "${{ steps.aggregate.outputs.devops_verdict }}" }
            @{ Name = 'üó∫Ô∏è Roadmap'; Verdict = "${{ steps.aggregate.outputs.roadmap_verdict }}" }
          )

          # Identify agents with blocking verdicts
          # NEEDS_REVIEW added in Issue #470 fix - indicates AI couldn't produce explicit verdict (occurs when AI response doesn't match expected verdict format, treating ambiguity as blocking)
          $blockingVerdicts = @('CRITICAL_FAIL', 'REJECTED', 'FAIL', 'NEEDS_REVIEW')
          $failedAgents = @()
          $missingVerdicts = @()

          foreach ($entry in $agentVerdicts) {
            if ([string]::IsNullOrWhiteSpace($entry.Verdict)) {
              $missingVerdicts += $entry.Name
              Write-Output "::error::$($entry.Name): No verdict received from aggregate step"
              continue
            }
            if ($entry.Verdict -in $blockingVerdicts) {
              $failedAgents += "$($entry.Name): $($entry.Verdict)"
              Write-Output "::error::$($entry.Name): $($entry.Verdict)"
            }
          }

          if ($missingVerdicts.Count -gt 0) {
            Write-Output ""
            Write-Output "‚ùå Quality gate failed - missing verdicts"
            Write-Output ""
            Write-Output "Agents with missing verdicts:"
            foreach ($missing in $missingVerdicts) {
              Write-Output "  - $missing"
            }
            Write-Output ""
            Write-Output "This indicates agent review jobs failed or artifacts are incomplete"
            exit 1
          }

          if ($finalVerdict -in $blockingVerdicts) {
            Write-Output ""
            Write-Output "‚ùå AI Quality Gate FAILED"
            Write-Output ""
            Write-Output "Agents with blocking verdicts:"
            foreach ($failed in $failedAgents) {
              Write-Output "  - $failed"
            }
            Write-Output ""
            Write-Output "Click on individual agent jobs above to see detailed findings."
            exit 1
          }

          Write-Output "‚úÖ AI Quality Gate passed with verdict: $finalVerdict"
