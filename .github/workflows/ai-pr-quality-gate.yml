name: AI PR Quality Gate

# AI-powered PR review using GitHub Copilot CLI

# Invokes security, qa, and analyst agents to review PRs IN PARALLEL

# Blocks merge if CRITICAL_FAIL detected

on:
  pull_request:
    branches: [main]
    types: [opened, synchronize, reopened]
    # NO paths filter - use dorny/paths-filter internally to satisfy required checks

  workflow_dispatch:
    inputs:
      pr_number:
        description: 'PR number to review'
        required: true
        type: number
      enable_debouncing:
        description: 'Enable debouncing delay to reduce race conditions (adds 10s latency)'
        required: false
        type: boolean
        default: false

permissions:
  contents: read
  pull-requests: write

# Concurrency control: Attempts to cancel in-progress runs when new runs start

# NOTE: GitHub Actions does NOT guarantee run coalescing - race conditions can occur

# where multiple runs start before cancellation takes effect. Multiple review runs

# may execute simultaneously, wasting compute until cancellation completes.

# See ADR-026 (../.agents/architecture/ADR-026-pr-automation-concurrency-and-safety.md)

# for architectural decision on workflow-level concurrency control

# Mitigation: Path filtering, timeouts, and PR-specific temp files reduce impact

concurrency:
  group: ai-quality-${{ github.event.pull_request.number || inputs.pr_number }}
  cancel-in-progress: true

env:

# Compute PR number from either pull_request event or workflow_dispatch input

  PR_NUMBER: ${{ github.event.pull_request.number || inputs.pr_number }}

jobs:
  # Optional debouncing job - runs only when enable_debouncing=true
  debounce:
    name: Debounce Workflow
    if: (github.event_name == 'workflow_dispatch' && inputs.enable_debouncing == true) || (github.event_name == 'pull_request' && vars.ENABLE_DEBOUNCE == 'true')
    runs-on: ubuntu-24.04-arm
    steps:
      - name: Checkout repository
        uses: actions/checkout@34e114876b0b11c390a56381ad16ebd13914f8d5
      
      - name: Apply debouncing delay
        uses: ./.github/actions/workflow-debounce
        with:
          delay-seconds: '10'
          workflow-name: 'AI PR Quality Gate'
          concurrency-group: ${{ github.event.pull_request.number || inputs.pr_number }}

# Check if relevant files changed using paths-filter

  check-changes:
    name: Check Changes
    # ADR-025: ARM runner for cost optimization (37.5% savings vs x64)
    runs-on: ubuntu-24.04-arm
    needs: debounce
    if: |-
      always() &&
      (needs.debounce.result == 'success' || needs.debounce.result == 'skipped') &&
      github.actor != 'dependabot[bot]' && github.actor != 'github-actions[bot]'

    outputs:
      # For workflow_dispatch, always run review (manual trigger assumes user intent regardless of files changed); otherwise use paths-filter result
      should-run: ${{ github.event_name == 'workflow_dispatch' && 'true' || steps.determine.outputs.should-run }}

    steps:
      - name: Checkout repository
        uses: actions/checkout@34e114876b0b11c390a56381ad16ebd13914f8d5

      - name: Check for relevant file changes
        uses: dorny/paths-filter@de90cc6fb38fc0963ad72b210f1f284cd68cea36 # v3
        id: filter
        # Skip paths-filter for workflow_dispatch (path filter requires pull_request event context; manual dispatch lacks base/head ref information)
        if: github.event_name != 'workflow_dispatch'
        with:
          filters: |
            relevant:
              - 'src/**'
              - 'scripts/**'
              - 'build/**'
              - '.github/**'
              - '.agents/**'
              - '.claude/skills/**'

      - name: Determine if review should run
        id: determine
        run: |
          if [ "${{ github.event_name }}" = "workflow_dispatch" ]; then
            echo "should-run=true" >> $GITHUB_OUTPUT
            echo "Decision: Run review (manual trigger)"
          else
            RELEVANT="${{ steps.filter.outputs.relevant }}"

            echo "Relevant files changed: $RELEVANT"

            # Run review if relevant files changed, skip otherwise
            if [ "$RELEVANT" = "true" ]; then
              echo "should-run=true" >> $GITHUB_OUTPUT
              echo "Decision: Run review (relevant files changed)"
            else
              echo "should-run=false" >> $GITHUB_OUTPUT
              echo "Decision: Skip review (no relevant files changed)"
            fi
          fi

# ==============================================================================
# Pre-execute Tests (Issue #77: QA agent cannot run tests directly)
# ==============================================================================
#
# The QA agent runs via Copilot CLI (text-in/text-out) with no shell access.
# This job executes tests and passes results as context to the QA agent,
# enabling empirical test verification in QA verdicts.

  run-tests:
    name: Run Tests
    # ADR-025: ARM runner for cost optimization (37.5% savings vs x64)
    runs-on: ubuntu-24.04-arm
    needs: check-changes
    if: always() && needs.check-changes.result == 'success' && needs.check-changes.outputs.should-run == 'true'
    timeout-minutes: 10
    permissions:
      contents: read

    outputs:
      test-summary: ${{ steps.summary.outputs.test_summary }}

    steps:
      - name: Checkout repository
        uses: actions/checkout@34e114876b0b11c390a56381ad16ebd13914f8d5

      - name: Setup code environment
        uses: ./.github/actions/setup-code-env
        with:
          gh-token: ${{ github.token }}
          enable-pester: 'true'
          enable-python: 'true'
          enable-git-hooks: 'false'

      - name: Run Pester tests
        id: pester
        shell: pwsh -NoProfile -Command "& '{0}'"
        continue-on-error: true
        run: |
          $ErrorActionPreference = 'Continue'

          try {
            # Pester is installed by setup-code-env action
            $config = [PesterConfiguration]::Default
            $config.Run.Path = './tests'
            $config.Run.Exit = $false
            $config.Output.Verbosity = 'Normal'
            $config.Run.PassThru = $true

            $result = Invoke-Pester -Configuration $config

            "pester_total=$($result.TotalCount)" >> $env:GITHUB_OUTPUT
            "pester_passed=$($result.PassedCount)" >> $env:GITHUB_OUTPUT
            "pester_failed=$($result.FailedCount)" >> $env:GITHUB_OUTPUT
            "pester_skipped=$($result.SkippedCount)" >> $env:GITHUB_OUTPUT
            "pester_duration=$([math]::Round($result.Duration.TotalSeconds, 1))" >> $env:GITHUB_OUTPUT

            if ($result.FailedCount -gt 0) {
              "pester_status=FAIL" >> $env:GITHUB_OUTPUT
              $failedNames = ($result.Failed | ForEach-Object { "  - $($_.ExpandedName)" }) -join "`n"
              # Limit output size to stay within GitHub Actions output constraints
              $maxFailureLength = 2000
              if ($failedNames.Length -gt $maxFailureLength) {
                $failedNames = $failedNames.Substring(0, $maxFailureLength) + "`n  ... (truncated)"
              }
              $delimiter = "EOF_PESTER_$(Get-Random)"
              "pester_failures<<$delimiter" >> $env:GITHUB_OUTPUT
              $failedNames >> $env:GITHUB_OUTPUT
              "$delimiter" >> $env:GITHUB_OUTPUT
            } else {
              "pester_status=PASS" >> $env:GITHUB_OUTPUT
            }
          }
          catch {
            "pester_status=ERROR" >> $env:GITHUB_OUTPUT
            "pester_error=$($_.Exception.Message -replace "`n",' ')" >> $env:GITHUB_OUTPUT
          }

      - name: Run pytest
        id: pytest
        continue-on-error: true
        shell: pwsh -NoProfile -Command "& '{0}'"
        run: |
          $ErrorActionPreference = 'Continue'

          $pythonAvailable = Get-Command python -ErrorAction SilentlyContinue
          $pyprojectExists = Test-Path "pyproject.toml"

          if ($pythonAvailable -and $pyprojectExists) {
            try {
              # Prefer uv (installed by setup-code-env), fall back to python -m pytest
              $uvAvailable = Get-Command uv -ErrorAction SilentlyContinue
              if ($uvAvailable) {
                $output = & uv run pytest --tb=short -q 2>&1 | Out-String
              } else {
                $output = & python -m pytest --tb=short -q 2>&1 | Out-String
              }
              $exitCode = $LASTEXITCODE
              Write-Output $output

              # Extract summary line for reporting context
              $summaryLine = ($output -split "`n" | Where-Object { $_ -match "passed|failed|error" } | Select-Object -Last 1)
              if (-not $summaryLine) {
                $summaryLine = "No test summary available"
              }
              "pytest_summary=$summaryLine" >> $env:GITHUB_OUTPUT

              # Use exit code as primary signal (not output parsing)
              if ($exitCode -ne 0) {
                "pytest_status=FAIL" >> $env:GITHUB_OUTPUT
              } else {
                "pytest_status=PASS" >> $env:GITHUB_OUTPUT
              }
            }
            catch {
              "pytest_status=ERROR" >> $env:GITHUB_OUTPUT
              "pytest_summary=$($_.Exception.Message -replace "`n",' ')" >> $env:GITHUB_OUTPUT
            }
          } else {
            "pytest_status=SKIPPED" >> $env:GITHUB_OUTPUT
            "pytest_summary=Python test environment not available" >> $env:GITHUB_OUTPUT
          }

      - name: Generate test summary
        id: summary
        shell: pwsh -NoProfile -Command "& '{0}'"
        env:
          PESTER_STATUS: ${{ steps.pester.outputs.pester_status || 'SKIPPED' }}
          PESTER_TOTAL: ${{ steps.pester.outputs.pester_total || '0' }}
          PESTER_PASSED: ${{ steps.pester.outputs.pester_passed || '0' }}
          PESTER_FAILED: ${{ steps.pester.outputs.pester_failed || '0' }}
          PESTER_SKIPPED: ${{ steps.pester.outputs.pester_skipped || '0' }}
          PESTER_DURATION: ${{ steps.pester.outputs.pester_duration || '0' }}
          PESTER_FAILURES: ${{ steps.pester.outputs.pester_failures || '' }}
          PESTER_ERROR: ${{ steps.pester.outputs.pester_error || '' }}
          PYTEST_STATUS: ${{ steps.pytest.outputs.pytest_status || 'SKIPPED' }}
          PYTEST_SUMMARY: ${{ steps.pytest.outputs.pytest_summary || 'Not executed' }}
        run: |
          $summary = "## Pre-executed Test Results`n`n"
          $summary += "### Pester Tests (PowerShell)`n"
          $summary += "- **Status**: $env:PESTER_STATUS`n"

          if ($env:PESTER_STATUS -notin @('SKIPPED', 'ERROR')) {
            $summary += "- **Total**: $env:PESTER_TOTAL | **Passed**: $env:PESTER_PASSED | **Failed**: $env:PESTER_FAILED | **Skipped**: $env:PESTER_SKIPPED`n"
            $summary += "- **Duration**: $($env:PESTER_DURATION)s`n"
          }

          if ($env:PESTER_STATUS -eq 'FAIL' -and $env:PESTER_FAILURES) {
            $summary += "`n**Failed Tests**:`n$env:PESTER_FAILURES`n"
          }
          elseif ($env:PESTER_STATUS -eq 'ERROR' -and $env:PESTER_ERROR) {
            $summary += "- **Error**: $env:PESTER_ERROR`n"
          }

          $summary += "`n### pytest (Python)`n"
          $summary += "- **Status**: $env:PYTEST_STATUS`n"
          $summary += "- **Summary**: $env:PYTEST_SUMMARY`n"

          $delimiter = "EOF_SUMMARY_$(Get-Random)"
          "test_summary<<$delimiter" >> $env:GITHUB_OUTPUT
          $summary >> $env:GITHUB_OUTPUT
          "$delimiter" >> $env:GITHUB_OUTPUT

          Write-Output "Generated test summary:"
          Write-Output $summary

# ==============================================================================
# Agent Review Jobs (6 Static Jobs)
# ==============================================================================
#
# Each agent (security, qa, analyst, architect, devops, roadmap) has a dedicated
# job with a static name. This ensures status check contexts are always emitted
# with proper names, even when reviews are skipped due to file filtering.
#
# Why not matrix? Job-level `if:` conditions prevent matrix expansion, leaving
# check context names as unresolved placeholders (e.g., "${{ matrix.agent }} Review").
# Static jobs with step-level conditionals solve this by:
# 1. Job names resolve at parse time (not runtime)
# 2. Jobs always run, emitting proper status checks
# 3. Composite action (.github/actions/agent-review) encapsulates common workflow
#
# Trade-off: Six job definitions (CVA) vs correctness (reliable status checks).

  security-review:
    name: Security Review
    runs-on: ubuntu-24.04-arm
    needs: check-changes
    if: always() && needs.check-changes.result == 'success'
    timeout-minutes: 10
    permissions:
      contents: read
      pull-requests: write

    steps:
      - name: Checkout repository
        uses: actions/checkout@34e114876b0b11c390a56381ad16ebd13914f8d5

      - name: Run security review
        uses: ./.github/actions/agent-review
        with:
          agent: security
          emoji: üîí
          prompt-file: .github/prompts/pr-quality-gate-security.md
          should-run: ${{ needs.check-changes.outputs.should-run }}
          pr-number: ${{ env.PR_NUMBER }}
          bot-pat: ${{ secrets.BOT_PAT }}
          copilot-token: ${{ secrets.COPILOT_GITHUB_TOKEN }}

  qa-review:
    name: QA Review
    runs-on: ubuntu-24.04-arm
    needs: [check-changes, run-tests]
    if: always() && needs.check-changes.result == 'success'
    timeout-minutes: 10
    permissions:
      contents: read
      pull-requests: write

    steps:
      - name: Checkout repository
        uses: actions/checkout@34e114876b0b11c390a56381ad16ebd13914f8d5

      - name: Run qa review
        uses: ./.github/actions/agent-review
        with:
          agent: qa
          emoji: üß™
          prompt-file: .github/prompts/pr-quality-gate-qa.md
          should-run: ${{ needs.check-changes.outputs.should-run }}
          pr-number: ${{ env.PR_NUMBER }}
          bot-pat: ${{ secrets.BOT_PAT }}
          copilot-token: ${{ secrets.COPILOT_GITHUB_TOKEN }}
          additional-context: ${{ needs.run-tests.outputs.test-summary || 'Test execution was skipped (no relevant file changes or test job did not run).' }}

  analyst-review:
    name: Analyst Review
    runs-on: ubuntu-24.04-arm
    needs: check-changes
    if: always() && needs.check-changes.result == 'success'
    timeout-minutes: 10
    permissions:
      contents: read
      pull-requests: write

    steps:
      - name: Checkout repository
        uses: actions/checkout@34e114876b0b11c390a56381ad16ebd13914f8d5

      - name: Run analyst review
        uses: ./.github/actions/agent-review
        with:
          agent: analyst
          emoji: üìä
          prompt-file: .github/prompts/pr-quality-gate-analyst.md
          should-run: ${{ needs.check-changes.outputs.should-run }}
          pr-number: ${{ env.PR_NUMBER }}
          bot-pat: ${{ secrets.BOT_PAT }}
          copilot-token: ${{ secrets.COPILOT_GITHUB_TOKEN }}

  architect-review:
    name: Architect Review
    runs-on: ubuntu-24.04-arm
    needs: check-changes
    if: always() && needs.check-changes.result == 'success'
    timeout-minutes: 10
    permissions:
      contents: read
      pull-requests: write

    steps:
      - name: Checkout repository
        uses: actions/checkout@34e114876b0b11c390a56381ad16ebd13914f8d5

      - name: Run architect review
        uses: ./.github/actions/agent-review
        with:
          agent: architect
          emoji: üìê
          prompt-file: .github/prompts/pr-quality-gate-architect.md
          should-run: ${{ needs.check-changes.outputs.should-run }}
          pr-number: ${{ env.PR_NUMBER }}
          bot-pat: ${{ secrets.BOT_PAT }}
          copilot-token: ${{ secrets.COPILOT_GITHUB_TOKEN }}

  devops-review:
    name: DevOps Review
    runs-on: ubuntu-24.04-arm
    needs: check-changes
    if: always() && needs.check-changes.result == 'success'
    timeout-minutes: 10
    permissions:
      contents: read
      pull-requests: write

    steps:
      - name: Checkout repository
        uses: actions/checkout@34e114876b0b11c390a56381ad16ebd13914f8d5

      - name: Run devops review
        uses: ./.github/actions/agent-review
        with:
          agent: devops
          emoji: ‚öôÔ∏è
          prompt-file: .github/prompts/pr-quality-gate-devops.md
          should-run: ${{ needs.check-changes.outputs.should-run }}
          pr-number: ${{ env.PR_NUMBER }}
          bot-pat: ${{ secrets.BOT_PAT }}
          copilot-token: ${{ secrets.COPILOT_GITHUB_TOKEN }}

  roadmap-review:
    name: Roadmap Review
    runs-on: ubuntu-24.04-arm
    needs: check-changes
    if: always() && needs.check-changes.result == 'success'
    timeout-minutes: 10
    permissions:
      contents: read
      pull-requests: write

    steps:
      - name: Checkout repository
        uses: actions/checkout@34e114876b0b11c390a56381ad16ebd13914f8d5

      - name: Run roadmap review
        uses: ./.github/actions/agent-review
        with:
          agent: roadmap
          emoji: üó∫Ô∏è
          prompt-file: .github/prompts/pr-quality-gate-roadmap.md
          should-run: ${{ needs.check-changes.outputs.should-run }}
          pr-number: ${{ env.PR_NUMBER }}
          bot-pat: ${{ secrets.BOT_PAT }}
          copilot-token: ${{ secrets.COPILOT_GITHUB_TOKEN }}

# ==============================================================================
# Aggregate Results from All Agent Reviews
# ==============================================================================
#
# Depends on all six agent review jobs explicitly (not matrix).
# Downloads artifacts using pattern `review-*` to gather all agent verdicts.

  aggregate:
    name: Aggregate Results
    # ADR-025: ARM runner for cost optimization (37.5% savings vs x64)
    runs-on: ubuntu-24.04-arm
    needs: [check-changes, security-review, qa-review, analyst-review, architect-review, devops-review, roadmap-review]
    # Always run if check-changes ran and said to run, even if some agent jobs failed
    if: always() && needs.check-changes.outputs.should-run == 'true'

    outputs:
      final-verdict: ${{ steps.aggregate.outputs.final_verdict }}

    steps:
      - name: Check for failed agents
        id: failed-agents
        shell: pwsh -NoProfile -Command "& '{0}'"
        env:
          SECURITY_RESULT: ${{ needs.security-review.result }}
          QA_RESULT: ${{ needs.qa-review.result }}
          ANALYST_RESULT: ${{ needs.analyst-review.result }}
          ARCHITECT_RESULT: ${{ needs.architect-review.result }}
          DEVOPS_RESULT: ${{ needs.devops-review.result }}
          ROADMAP_RESULT: ${{ needs.roadmap-review.result }}
        run: |
          $ErrorActionPreference = 'Stop'
          # Check if any agent review jobs failed by examining the needs context
          # This step runs before downloading artifacts to surface failures early
          # Note: Specific failed agents are identified later by examining individual verdicts (line 777+ in 'Check for Critical Failures' step)
          $agentResults = @(
            @{ Name = 'Security'; Result = $env:SECURITY_RESULT }
            @{ Name = 'QA'; Result = $env:QA_RESULT }
            @{ Name = 'Analyst'; Result = $env:ANALYST_RESULT }
            @{ Name = 'Architect'; Result = $env:ARCHITECT_RESULT }
            @{ Name = 'DevOps'; Result = $env:DEVOPS_RESULT }
            @{ Name = 'Roadmap'; Result = $env:ROADMAP_RESULT }
          )

          Write-Output "Individual review results:"
          $failedAgents = @()

          foreach ($agent in $agentResults) {
            Write-Output "  $($agent.Name): $($agent.Result)"
            if ($agent.Result -in @('failure', 'cancelled')) {
              $failedAgents += $agent.Name
              Write-Output "::error::$($agent.Name) agent failed with result: $($agent.Result)"
            }
          }

          if ($failedAgents.Count -gt 0) {
            "has_failures=true" >> $env:GITHUB_OUTPUT
            Write-Output "::warning::Failed agents: $($failedAgents -join ', ')"
            Write-Output "::warning::Check individual job logs for details"
          } else {
            "has_failures=false" >> $env:GITHUB_OUTPUT
          }

      - name: Checkout repository
        uses: actions/checkout@34e114876b0b11c390a56381ad16ebd13914f8d5

      - name: Download all review artifacts
        uses: actions/download-artifact@d3f86a106a0bac45b974a628896c90dbdf5c8093
        with:
          pattern: review-*
          path: ai-review-results
          merge-multiple: true

      - name: Validate artifact download
        shell: pwsh -NoProfile -Command "& '{0}'"
        run: |
          $ErrorActionPreference = 'Stop'
          Write-Output "Checking for required verdict files..."

          $agents = @('security', 'qa', 'analyst', 'architect', 'devops', 'roadmap')
          $missingFiles = @()

          foreach ($agent in $agents) {
            $verdictFile = "ai-review-results/$agent-verdict.txt"
            if (-not (Test-Path $verdictFile)) {
              $missingFiles += "$agent-verdict.txt"
            }
          }

          if ($missingFiles.Count -gt 0) {
            Write-Output "::error::Artifact download incomplete - missing files:"
            foreach ($file in $missingFiles) {
              Write-Output "::error::  - $file"
            }
            Write-Output "::error::This indicates agent review jobs failed or artifacts were not uploaded"
            exit 1
          }

          Write-Output "‚úì All required verdict files present"

      - name: Load review results
        id: load-results
        shell: pwsh -NoProfile -Command "& '{0}'"
        run: |
          $ErrorActionPreference = 'Stop'
          $agents = @('security', 'qa', 'analyst', 'architect', 'devops', 'roadmap')

          Write-Output "Loaded verdicts:"

          foreach ($agent in $agents) {
            $verdictFile = "ai-review-results/$agent-verdict.txt"
            $infraFile = "ai-review-results/$agent-infrastructure-failure.txt"

            # Read verdict (file existence already validated)
            $verdict = (Get-Content $verdictFile -Raw).Trim()

            # Read infrastructure flag
            $infra = if (Test-Path $infraFile) { (Get-Content $infraFile -Raw).Trim() } else { "false" }

            Write-Output "  ${agent}: $verdict (infra: $infra)"

            # Export to GITHUB_OUTPUT
            "${agent}_verdict=$verdict" >> $env:GITHUB_OUTPUT
            "${agent}_infra=$infra" >> $env:GITHUB_OUTPUT
          }

      - name: Aggregate Verdicts
        id: aggregate
        env:
          SECURITY_VERDICT: ${{ steps.load-results.outputs.security_verdict }}
          QA_VERDICT: ${{ steps.load-results.outputs.qa_verdict }}
          ANALYST_VERDICT: ${{ steps.load-results.outputs.analyst_verdict }}
          ARCHITECT_VERDICT: ${{ steps.load-results.outputs.architect_verdict }}
          DEVOPS_VERDICT: ${{ steps.load-results.outputs.devops_verdict }}
          ROADMAP_VERDICT: ${{ steps.load-results.outputs.roadmap_verdict }}
          SECURITY_INFRA: ${{ steps.load-results.outputs.security_infra }}
          QA_INFRA: ${{ steps.load-results.outputs.qa_infra }}
          ANALYST_INFRA: ${{ steps.load-results.outputs.analyst_infra }}
          ARCHITECT_INFRA: ${{ steps.load-results.outputs.architect_infra }}
          DEVOPS_INFRA: ${{ steps.load-results.outputs.devops_infra }}
          ROADMAP_INFRA: ${{ steps.load-results.outputs.roadmap_infra }}
        run: python3 .github/scripts/aggregate_quality_verdicts.py

      - name: Generate Report
        id: report
        env:
          RUN_ID: ${{ github.run_id }}
          SERVER_URL: ${{ github.server_url }}
          REPOSITORY: ${{ github.repository }}
          EVENT_NAME: ${{ github.event_name }}
          REF_NAME: ${{ github.ref_name }}
          SHA: ${{ github.sha }}
          FINAL_VERDICT: ${{ steps.aggregate.outputs.final_verdict }}
          SECURITY_VERDICT: ${{ steps.aggregate.outputs.security_verdict }}
          QA_VERDICT: ${{ steps.aggregate.outputs.qa_verdict }}
          ANALYST_VERDICT: ${{ steps.aggregate.outputs.analyst_verdict }}
          ARCHITECT_VERDICT: ${{ steps.aggregate.outputs.architect_verdict }}
          DEVOPS_VERDICT: ${{ steps.aggregate.outputs.devops_verdict }}
          ROADMAP_VERDICT: ${{ steps.aggregate.outputs.roadmap_verdict }}
          SECURITY_CATEGORY: ${{ steps.aggregate.outputs.security_category }}
          QA_CATEGORY: ${{ steps.aggregate.outputs.qa_category }}
          ANALYST_CATEGORY: ${{ steps.aggregate.outputs.analyst_category }}
          ARCHITECT_CATEGORY: ${{ steps.aggregate.outputs.architect_category }}
          DEVOPS_CATEGORY: ${{ steps.aggregate.outputs.devops_category }}
          ROADMAP_CATEGORY: ${{ steps.aggregate.outputs.roadmap_category }}
        run: python3 .github/scripts/generate_quality_report.py

      - name: Check for infrastructure failures and add label
        shell: pwsh -NoProfile -Command "& '{0}'"
        env:
          GH_TOKEN: ${{ github.token }}
          PR_NUMBER: ${{ env.PR_NUMBER }}
        run: |
          $ErrorActionPreference = 'Stop'

          # Check if any agent had infrastructure failure (Issue #328: Add infrastructure-failure label when retry logic exhausted)
          $infrastructureFailureDetected = $false
          $retryAttempts = @()

          $agents = @('security', 'qa', 'analyst', 'architect', 'devops', 'roadmap')

          foreach ($agent in $agents) {
            $infraFile = "ai-review-results/$agent-infrastructure-failure.txt"
            $retryFile = "ai-review-results/$agent-retry-count.txt"

            if (Test-Path $infraFile) {
              $infraFlag = (Get-Content $infraFile -Raw).Trim()
              if ($infraFlag -eq "true") {
                $infrastructureFailureDetected = $true
                $retryCount = 0
                if (Test-Path $retryFile) {
                  $retryCount = [int](Get-Content $retryFile -Raw).Trim()
                }
                $retryAttempts += "${agent}: $retryCount retries"
                Write-Output "::notice::Infrastructure failure detected for $agent agent (retries: $retryCount)"
              }
            }
          }

          if ($infrastructureFailureDetected) {
            Write-Output "::warning::Infrastructure failures detected - adding infrastructure-failure label"
            Write-Output "Retry attempts: $($retryAttempts -join ', ')"

            try {
              # Verify gh CLI is authenticated
              $authStatus = gh auth status 2>&1
              if ($LASTEXITCODE -ne 0) {
                Write-Output "::error::gh CLI authentication failed: $authStatus"
                Write-Output "::error::Cannot add infrastructure-failure label"
                exit 1
              }

              # Add label with error handling
              gh pr edit $env:PR_NUMBER --repo $env:GITHUB_REPOSITORY --add-label "infrastructure-failure"

              if ($LASTEXITCODE -ne 0) {
                Write-Output "::error::Failed to add infrastructure-failure label (exit code: $LASTEXITCODE)"
                Write-Output "::error::Ensure 'infrastructure-failure' label exists in repository"
                exit 1
              }

              Write-Output "‚úì Successfully added infrastructure-failure label"
            }
            catch {
              Write-Output "::error::Failed to add label: $_"
              exit 1
            }
          } else {
            Write-Output "No infrastructure failures detected"
          }

      - name: Post PR Comment
        shell: pwsh -NoProfile -Command "& '{0}'"
        env:
          GH_TOKEN: ${{ github.token }}
          PR_NUMBER: ${{ env.PR_NUMBER }}
          REPORT_FILE: ${{ steps.report.outputs.report_file }}
        run: |
          $ErrorActionPreference = 'Stop'

          if ([string]::IsNullOrWhiteSpace($env:PR_NUMBER)) {
            Write-Output "::error::PR_NUMBER environment variable is missing"
            exit 1
          }

          if ([string]::IsNullOrWhiteSpace($env:REPORT_FILE)) {
            Write-Output "::error::REPORT_FILE environment variable is missing"
            exit 1
          }

          if (-not (Test-Path $env:REPORT_FILE)) {
            Write-Output "::error::Report file not found: $env:REPORT_FILE"
            exit 1
          }

          $skillScript = ".claude/skills/github/scripts/issue/Post-IssueComment.ps1"
          if (-not (Test-Path $skillScript)) {
            Write-Output "::error::Skill script not found: $skillScript"
            exit 1
          }

          try {
            # Use GitHub skill script for idempotent comment posting
            # PRs are issues in GitHub API (pull requests are a superset of issues), so we use Post-IssueComment with marker `AI-PR-QUALITY-GATE`
            # UpdateIfExists updates the existing marked comment if present, ensuring the latest report is reflected
            & $skillScript `
              -Issue $env:PR_NUMBER `
              -BodyFile $env:REPORT_FILE `
              -Marker "AI-PR-QUALITY-GATE" `
              -UpdateIfExists

            if ($LASTEXITCODE -ne 0) {
              Write-Output "::error::Post-IssueComment.ps1 failed with exit code $LASTEXITCODE"
              exit $LASTEXITCODE
            }
          }
          catch {
            Write-Output "::error::Failed to post PR comment: $_"
            exit 1
          }

      - name: Set Job Summary
        shell: pwsh -NoProfile -Command "& '{0}'"
        env:
          REPORT_FILE: ${{ steps.report.outputs.report_file }}
        run: |
          $ErrorActionPreference = 'Stop'
          Get-Content $env:REPORT_FILE | Add-Content $env:GITHUB_STEP_SUMMARY

      - name: Check for Critical Failures
        shell: pwsh -NoProfile -Command "& '{0}'"
        env:
          FINAL_VERDICT: ${{ steps.aggregate.outputs.final_verdict }}
          SECURITY_VERDICT: ${{ steps.aggregate.outputs.security_verdict }}
          QA_VERDICT: ${{ steps.aggregate.outputs.qa_verdict }}
          ANALYST_VERDICT: ${{ steps.aggregate.outputs.analyst_verdict }}
          ARCHITECT_VERDICT: ${{ steps.aggregate.outputs.architect_verdict }}
          DEVOPS_VERDICT: ${{ steps.aggregate.outputs.devops_verdict }}
          ROADMAP_VERDICT: ${{ steps.aggregate.outputs.roadmap_verdict }}
        run: |
          $ErrorActionPreference = 'Stop'
          $finalVerdict = $env:FINAL_VERDICT

          # Use ordered array for deterministic output (PowerShell dictionary Keys property is unordered, which would cause agent order to vary across runs, complicating comparisons)
          $agentVerdicts = @(
            @{ Name = 'üîí Security'; Verdict = $env:SECURITY_VERDICT }
            @{ Name = 'üß™ QA'; Verdict = $env:QA_VERDICT }
            @{ Name = 'üìä Analyst'; Verdict = $env:ANALYST_VERDICT }
            @{ Name = 'üìê Architect'; Verdict = $env:ARCHITECT_VERDICT }
            @{ Name = '‚öôÔ∏è DevOps'; Verdict = $env:DEVOPS_VERDICT }
            @{ Name = 'üó∫Ô∏è Roadmap'; Verdict = $env:ROADMAP_VERDICT }
          )

          # Identify agents with blocking verdicts
          # NEEDS_REVIEW added in Issue #470 fix - indicates AI couldn't produce explicit verdict (occurs when AI response doesn't match expected verdict format, treating ambiguity as blocking)
          $blockingVerdicts = @('CRITICAL_FAIL', 'REJECTED', 'FAIL', 'NEEDS_REVIEW')
          $failedAgents = @()
          $missingVerdicts = @()

          foreach ($entry in $agentVerdicts) {
            if ([string]::IsNullOrWhiteSpace($entry.Verdict)) {
              $missingVerdicts += $entry.Name
              Write-Output "::error::$($entry.Name): No verdict received from aggregate step"
              continue
            }
            if ($entry.Verdict -in $blockingVerdicts) {
              $failedAgents += "$($entry.Name): $($entry.Verdict)"
              Write-Output "::error::$($entry.Name): $($entry.Verdict)"
            }
          }

          if ($missingVerdicts.Count -gt 0) {
            Write-Output ""
            Write-Output "‚ùå Quality gate failed - missing verdicts"
            Write-Output ""
            Write-Output "Agents with missing verdicts:"
            foreach ($missing in $missingVerdicts) {
              Write-Output "  - $missing"
            }
            Write-Output ""
            Write-Output "This indicates agent review jobs failed or artifacts are incomplete"
            exit 1
          }

          if ($finalVerdict -in $blockingVerdicts) {
            Write-Output ""
            Write-Output "‚ùå AI Quality Gate FAILED"
            Write-Output ""
            Write-Output "Agents with blocking verdicts:"
            foreach ($failed in $failedAgents) {
              Write-Output "  - $failed"
            }
            Write-Output ""
            Write-Output "Click on individual agent jobs above to see detailed findings."
            exit 1
          }

          Write-Output "‚úÖ AI Quality Gate passed with verdict: $finalVerdict"
