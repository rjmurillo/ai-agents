name: AI PR Quality Gate

# AI-powered PR review using GitHub Copilot CLI

# Invokes security, qa, and analyst agents to review PRs IN PARALLEL

# Blocks merge if CRITICAL_FAIL detected

on:
  pull_request:
    branches: [main]
    types: [opened, synchronize, reopened]
    # NO paths filter - use dorny/paths-filter internally to satisfy required checks

  workflow_dispatch:
    inputs:
      pr_number:
        description: 'PR number to review'
        required: true
        type: number
      enable_debouncing:
        description: 'Enable debouncing delay to reduce race conditions (adds 10s latency)'
        required: false
        type: boolean
        default: false
      bypass-cache:
        description: 'Bypass cached review results and force fresh Copilot API calls'
        required: false
        type: boolean
        default: false

permissions:
  contents: read
  pull-requests: write

# Concurrency control: Attempts to cancel in-progress runs when new runs start

# NOTE: GitHub Actions does NOT guarantee run coalescing - race conditions can occur

# where multiple runs start before cancellation takes effect. Multiple review runs

# may execute simultaneously, wasting compute until cancellation completes.

# See ADR-026 (../.agents/architecture/ADR-026-pr-automation-concurrency-and-safety.md)

# for architectural decision on workflow-level concurrency control

# Mitigation: Path filtering, timeouts, and PR-specific temp files reduce impact

concurrency:
  group: ai-quality-${{ github.event.pull_request.number || inputs.pr_number }}
  cancel-in-progress: true

env:

# Compute PR number from either pull_request event or workflow_dispatch input

  PR_NUMBER: ${{ github.event.pull_request.number || inputs.pr_number }}

jobs:
  # Optional debouncing job - runs only when enable_debouncing=true
  debounce:
    name: Debounce Workflow
    if: (github.event_name == 'workflow_dispatch' && inputs.enable_debouncing == true) || (github.event_name == 'pull_request' && vars.ENABLE_DEBOUNCE == 'true')
    runs-on: ubuntu-24.04-arm
    steps:
      - name: Checkout repository
        uses: actions/checkout@34e114876b0b11c390a56381ad16ebd13914f8d5
      
      - name: Apply debouncing delay
        uses: ./.github/actions/workflow-debounce
        with:
          delay-seconds: '10'
          workflow-name: 'AI PR Quality Gate'
          concurrency-group: ${{ github.event.pull_request.number || inputs.pr_number }}

# Check if relevant files changed using paths-filter

  check-changes:
    name: Check Changes
    # ADR-025: ARM runner for cost optimization (37.5% savings vs x64)
    runs-on: ubuntu-24.04-arm
    needs: debounce
    if: |-
      always() &&
      (needs.debounce.result == 'success' || needs.debounce.result == 'skipped') &&
      github.actor != 'dependabot[bot]' && github.actor != 'github-actions[bot]'

    outputs:
      # For workflow_dispatch, always run review (manual trigger assumes user intent regardless of files changed); otherwise use paths-filter result
      should-run: ${{ github.event_name == 'workflow_dispatch' && 'true' || steps.determine.outputs.should-run }}

    steps:
      - name: Checkout repository
        uses: actions/checkout@34e114876b0b11c390a56381ad16ebd13914f8d5

      - name: Check for relevant file changes
        uses: dorny/paths-filter@de90cc6fb38fc0963ad72b210f1f284cd68cea36 # v3
        id: filter
        # Skip paths-filter for workflow_dispatch (path filter requires pull_request event context; manual dispatch lacks base/head ref information)
        if: github.event_name != 'workflow_dispatch'
        with:
          filters: |
            relevant:
              - 'src/**'
              - 'scripts/**'
              - 'build/**'
              - '.github/**'
              - '.agents/**'
              - '!.agents/planning/**'
              - '!.agents/sessions/**'
              - '.claude/skills/**'

      - name: Determine if review should run
        id: determine
        run: |
          if [ "${{ github.event_name }}" = "workflow_dispatch" ]; then
            echo "should-run=true" >> $GITHUB_OUTPUT
            echo "Decision: Run review (manual trigger)"
          else
            RELEVANT="${{ steps.filter.outputs.relevant }}"

            echo "Relevant files changed: $RELEVANT"

            # Run review if relevant files changed, skip otherwise
            if [ "$RELEVANT" = "true" ]; then
              echo "should-run=true" >> $GITHUB_OUTPUT
              echo "Decision: Run review (relevant files changed)"
            else
              echo "should-run=false" >> $GITHUB_OUTPUT
              echo "Decision: Skip review (no relevant files changed)"
            fi
          fi

# ==============================================================================
# Infrastructure Health Check (Issue #153)
# ==============================================================================
#
# Lightweight, zero-cost validation of agent prerequisites.
# Distinguishes environment errors from code quality issues.
# When Copilot CLI is unavailable, agents are skipped (not failed).

  infra-check:
    name: Infrastructure Check
    runs-on: ubuntu-24.04-arm
    needs: check-changes
    if: always() && needs.check-changes.result == 'success' && needs.check-changes.outputs.should-run == 'true'
    timeout-minutes: 2
    permissions:
      contents: read

    outputs:
      copilot-available: ${{ steps.infra.outputs.copilot-available }}
      overall-status: ${{ steps.infra.outputs.overall-status }}

    steps:
      - name: Checkout repository
        uses: actions/checkout@34e114876b0b11c390a56381ad16ebd13914f8d5

      - name: Check agent infrastructure
        id: infra
        uses: ./.github/actions/check-agent-infrastructure
        with:
          bot-pat: ${{ secrets.BOT_PAT }}
          copilot-token: ${{ secrets.COPILOT_GITHUB_TOKEN }}

      - name: Report infrastructure status
        if: steps.infra.outputs.overall-status != 'ready'
        shell: bash
        env:
          INFRA_STATUS: ${{ steps.infra.outputs.overall-status }}
          COPILOT_STATUS: ${{ steps.infra.outputs.copilot-available }}
        run: |
          echo "::warning::Agent infrastructure is $INFRA_STATUS."
          echo "::warning::Copilot available: $COPILOT_STATUS"
          echo "::warning::Agent reviews requiring Copilot CLI will be skipped."

# ==============================================================================
# Pre-execute Tests (Issue #77: QA agent cannot run tests directly)
# ==============================================================================
#
# The QA agent runs via Copilot CLI (text-in/text-out) with no shell access.
# This job executes tests and passes results as context to the QA agent,
# enabling empirical test verification in QA verdicts.

  run-tests:
    name: Run Tests
    # ADR-025: ARM runner for cost optimization (37.5% savings vs x64)
    runs-on: ubuntu-24.04-arm
    needs: check-changes
    if: always() && needs.check-changes.result == 'success' && needs.check-changes.outputs.should-run == 'true'
    timeout-minutes: 10
    permissions:
      contents: read

    outputs:
      test-summary: ${{ steps.summary.outputs.test_summary }}

    steps:
      - name: Checkout repository
        uses: actions/checkout@34e114876b0b11c390a56381ad16ebd13914f8d5

      - name: Setup code environment
        uses: ./.github/actions/setup-code-env
        with:
          gh-token: ${{ github.token }}
          enable-python: 'true'
          enable-git-hooks: 'false'

      - name: Run pytest
        id: pytest
        continue-on-error: true
        shell: pwsh -NoProfile -Command "& '{0}'"
        run: |
          $ErrorActionPreference = 'Continue'

          $pythonAvailable = Get-Command python -ErrorAction SilentlyContinue
          $pyprojectExists = Test-Path "pyproject.toml"

          if ($pythonAvailable -and $pyprojectExists) {
            try {
              # Prefer uv (installed by setup-code-env), fall back to python -m pytest
              $uvAvailable = Get-Command uv -ErrorAction SilentlyContinue
              if ($uvAvailable) {
                $output = & uv run pytest --tb=short -q 2>&1 | Out-String
              } else {
                $output = & python -m pytest --tb=short -q 2>&1 | Out-String
              }
              $exitCode = $LASTEXITCODE
              Write-Output $output

              # Extract summary line for reporting context
              $summaryLine = ($output -split "`n" | Where-Object { $_ -match "passed|failed|error" } | Select-Object -Last 1)
              if (-not $summaryLine) {
                $summaryLine = "No test summary available"
              }
              "pytest_summary=$summaryLine" >> $env:GITHUB_OUTPUT

              # Use exit code as primary signal (not output parsing)
              if ($exitCode -ne 0) {
                "pytest_status=FAIL" >> $env:GITHUB_OUTPUT
              } else {
                "pytest_status=PASS" >> $env:GITHUB_OUTPUT
              }
            }
            catch {
              "pytest_status=ERROR" >> $env:GITHUB_OUTPUT
              "pytest_summary=$($_.Exception.Message -replace "`n",' ')" >> $env:GITHUB_OUTPUT
            }
          } else {
            "pytest_status=SKIPPED" >> $env:GITHUB_OUTPUT
            "pytest_summary=Python test environment not available" >> $env:GITHUB_OUTPUT
          }

      - name: Generate test summary
        id: summary
        shell: python3 {0}
        env:
          PYTEST_STATUS: ${{ steps.pytest.outputs.pytest_status || 'SKIPPED' }}
          PYTEST_SUMMARY: ${{ steps.pytest.outputs.pytest_summary || 'Not executed' }}
        run: |
          import os
          import random

          pytest_status = os.environ.get('PYTEST_STATUS', 'SKIPPED')
          pytest_summary = os.environ.get('PYTEST_SUMMARY', 'Not executed')

          summary = "## Pre-executed Test Results\n\n"
          summary += "### pytest (Python)\n"
          summary += f"- **Status**: {pytest_status}\n"
          summary += f"- **Summary**: {pytest_summary}\n"

          # Write to GITHUB_OUTPUT using heredoc delimiter
          delimiter = f"EOF_SUMMARY_{random.randint(1000, 9999)}"
          with open(os.environ['GITHUB_OUTPUT'], 'a') as f:
              f.write(f"test_summary<<{delimiter}\n")
              f.write(summary)
              f.write(f"{delimiter}\n")

          print("Generated test summary:")
          print(summary)

# ==============================================================================
# Agent Review Jobs (6 Static Jobs)
# ==============================================================================
#
# Each agent (security, qa, analyst, architect, devops, roadmap) has a dedicated
# job with a static name. This ensures status check contexts are always emitted
# with proper names, even when reviews are skipped due to file filtering.
#
# Why not matrix? Job-level `if:` conditions prevent matrix expansion, leaving
# check context names as unresolved placeholders (e.g., "${{ matrix.agent }} Review").
# Static jobs with step-level conditionals solve this by:
# 1. Job names resolve at parse time (not runtime)
# 2. Jobs always run, emitting proper status checks
# 3. Composite action (.github/actions/agent-review) encapsulates common workflow
#
# Trade-off: Six job definitions (CVA) vs correctness (reliable status checks).

  security-review:
    name: Security Review
    runs-on: ubuntu-24.04-arm
    needs: [check-changes, infra-check]
    if: always() && needs.check-changes.result == 'success'
    timeout-minutes: 10
    permissions:
      contents: read
      pull-requests: write

    steps:
      - name: Checkout repository
        uses: actions/checkout@34e114876b0b11c390a56381ad16ebd13914f8d5

      - name: Run security review
        uses: ./.github/actions/agent-review
        with:
          agent: security
          emoji: üîí
          prompt-file: .github/prompts/pr-quality-gate-security.md
          should-run: ${{ needs.check-changes.outputs.should-run == 'true' && needs.infra-check.outputs.copilot-available == 'true' }}
          pr-number: ${{ env.PR_NUMBER }}
          bot-pat: ${{ secrets.BOT_PAT }}
          copilot-token: ${{ secrets.COPILOT_GITHUB_TOKEN }}
          commit-sha: ${{ github.event.pull_request.head.sha || github.sha }}
          bypass-cache: ${{ inputs.bypass-cache == true && 'true' || 'false' }}

  qa-review:
    name: QA Review
    runs-on: ubuntu-24.04-arm
    needs: [check-changes, infra-check, run-tests]
    if: always() && needs.check-changes.result == 'success'
    timeout-minutes: 10
    permissions:
      contents: read
      pull-requests: write

    steps:
      - name: Checkout repository
        uses: actions/checkout@34e114876b0b11c390a56381ad16ebd13914f8d5

      - name: Run qa review
        uses: ./.github/actions/agent-review
        with:
          agent: qa
          emoji: üß™
          prompt-file: .github/prompts/pr-quality-gate-qa.md
          should-run: ${{ needs.check-changes.outputs.should-run == 'true' && needs.infra-check.outputs.copilot-available == 'true' }}
          pr-number: ${{ env.PR_NUMBER }}
          bot-pat: ${{ secrets.BOT_PAT }}
          copilot-token: ${{ secrets.COPILOT_GITHUB_TOKEN }}
          additional-context: ${{ needs.run-tests.outputs.test-summary || 'Test execution was skipped (no relevant file changes or test job did not run).' }}
          commit-sha: ${{ github.event.pull_request.head.sha || github.sha }}
          bypass-cache: ${{ inputs.bypass-cache == true && 'true' || 'false' }}

  analyst-review:
    name: Analyst Review
    runs-on: ubuntu-24.04-arm
    needs: [check-changes, infra-check]
    if: always() && needs.check-changes.result == 'success'
    timeout-minutes: 10
    permissions:
      contents: read
      pull-requests: write

    steps:
      - name: Checkout repository
        uses: actions/checkout@34e114876b0b11c390a56381ad16ebd13914f8d5

      - name: Run analyst review
        uses: ./.github/actions/agent-review
        with:
          agent: analyst
          emoji: üìä
          prompt-file: .github/prompts/pr-quality-gate-analyst.md
          should-run: ${{ needs.check-changes.outputs.should-run == 'true' && needs.infra-check.outputs.copilot-available == 'true' }}
          pr-number: ${{ env.PR_NUMBER }}
          bot-pat: ${{ secrets.BOT_PAT }}
          copilot-token: ${{ secrets.COPILOT_GITHUB_TOKEN }}
          commit-sha: ${{ github.event.pull_request.head.sha || github.sha }}
          bypass-cache: ${{ inputs.bypass-cache == true && 'true' || 'false' }}

  architect-review:
    name: Architect Review
    runs-on: ubuntu-24.04-arm
    needs: [check-changes, infra-check]
    if: always() && needs.check-changes.result == 'success'
    timeout-minutes: 10
    permissions:
      contents: read
      pull-requests: write

    steps:
      - name: Checkout repository
        uses: actions/checkout@34e114876b0b11c390a56381ad16ebd13914f8d5

      - name: Run architect review
        uses: ./.github/actions/agent-review
        with:
          agent: architect
          emoji: üìê
          prompt-file: .github/prompts/pr-quality-gate-architect.md
          should-run: ${{ needs.check-changes.outputs.should-run == 'true' && needs.infra-check.outputs.copilot-available == 'true' }}
          pr-number: ${{ env.PR_NUMBER }}
          bot-pat: ${{ secrets.BOT_PAT }}
          copilot-token: ${{ secrets.COPILOT_GITHUB_TOKEN }}
          commit-sha: ${{ github.event.pull_request.head.sha || github.sha }}
          bypass-cache: ${{ inputs.bypass-cache == true && 'true' || 'false' }}

  devops-review:
    name: DevOps Review
    runs-on: ubuntu-24.04-arm
    needs: [check-changes, infra-check]
    if: always() && needs.check-changes.result == 'success'
    timeout-minutes: 10
    permissions:
      contents: read
      pull-requests: write

    steps:
      - name: Checkout repository
        uses: actions/checkout@34e114876b0b11c390a56381ad16ebd13914f8d5

      - name: Run devops review
        uses: ./.github/actions/agent-review
        with:
          agent: devops
          emoji: ‚öôÔ∏è
          prompt-file: .github/prompts/pr-quality-gate-devops.md
          should-run: ${{ needs.check-changes.outputs.should-run == 'true' && needs.infra-check.outputs.copilot-available == 'true' }}
          pr-number: ${{ env.PR_NUMBER }}
          bot-pat: ${{ secrets.BOT_PAT }}
          copilot-token: ${{ secrets.COPILOT_GITHUB_TOKEN }}
          commit-sha: ${{ github.event.pull_request.head.sha || github.sha }}
          bypass-cache: ${{ inputs.bypass-cache == true && 'true' || 'false' }}

  roadmap-review:
    name: Roadmap Review
    runs-on: ubuntu-24.04-arm
    needs: [check-changes, infra-check]
    if: always() && needs.check-changes.result == 'success'
    timeout-minutes: 10
    permissions:
      contents: read
      pull-requests: write

    steps:
      - name: Checkout repository
        uses: actions/checkout@34e114876b0b11c390a56381ad16ebd13914f8d5

      - name: Run roadmap review
        uses: ./.github/actions/agent-review
        with:
          agent: roadmap
          emoji: üó∫Ô∏è
          prompt-file: .github/prompts/pr-quality-gate-roadmap.md
          should-run: ${{ needs.check-changes.outputs.should-run == 'true' && needs.infra-check.outputs.copilot-available == 'true' }}
          pr-number: ${{ env.PR_NUMBER }}
          bot-pat: ${{ secrets.BOT_PAT }}
          copilot-token: ${{ secrets.COPILOT_GITHUB_TOKEN }}
          commit-sha: ${{ github.event.pull_request.head.sha || github.sha }}
          bypass-cache: ${{ inputs.bypass-cache == true && 'true' || 'false' }}

# ==============================================================================
# Aggregate Results from All Agent Reviews
# ==============================================================================
#
# Depends on all six agent review jobs explicitly (not matrix).
# Downloads artifacts using pattern `review-*` to gather all agent verdicts.

  # Pass-through job: satisfies required "Aggregate Results" check when path
  # filter skips the real aggregate job. GitHub branch protection requires
  # SUCCESS (not SKIPPED) for required checks. See issue #1168.
  aggregate-skip:
    name: Aggregate Results
    runs-on: ubuntu-24.04-arm
    needs: [check-changes]
    if: always() && needs.check-changes.result == 'success' && needs.check-changes.outputs.should-run != 'true'
    steps:
      - name: No relevant changes
        run: echo "Skipped - no relevant file changes detected"

  aggregate:
    name: Aggregate Results
    # ADR-025: ARM runner for cost optimization (37.5% savings vs x64)
    runs-on: ubuntu-24.04-arm
    needs: [check-changes, infra-check, security-review, qa-review, analyst-review, architect-review, devops-review, roadmap-review]
    # Always run if check-changes ran and said to run, even if some agent jobs failed
    if: always() && needs.check-changes.outputs.should-run == 'true'

    outputs:
      final-verdict: ${{ steps.aggregate.outputs.final_verdict }}

    steps:
      - name: Check for failed agents
        id: failed-agents
        shell: pwsh -NoProfile -Command "& '{0}'"
        env:
          SECURITY_RESULT: ${{ needs.security-review.result }}
          QA_RESULT: ${{ needs.qa-review.result }}
          ANALYST_RESULT: ${{ needs.analyst-review.result }}
          ARCHITECT_RESULT: ${{ needs.architect-review.result }}
          DEVOPS_RESULT: ${{ needs.devops-review.result }}
          ROADMAP_RESULT: ${{ needs.roadmap-review.result }}
        run: |
          $ErrorActionPreference = 'Stop'
          # Check if any agent review jobs failed by examining the needs context
          # This step runs before downloading artifacts to surface failures early
          # Note: Specific failed agents are identified later by examining individual verdicts (line 777+ in 'Check for Critical Failures' step)
          $agentResults = @(
            @{ Name = 'Security'; Result = $env:SECURITY_RESULT }
            @{ Name = 'QA'; Result = $env:QA_RESULT }
            @{ Name = 'Analyst'; Result = $env:ANALYST_RESULT }
            @{ Name = 'Architect'; Result = $env:ARCHITECT_RESULT }
            @{ Name = 'DevOps'; Result = $env:DEVOPS_RESULT }
            @{ Name = 'Roadmap'; Result = $env:ROADMAP_RESULT }
          )

          Write-Output "Individual review results:"
          $failedAgents = @()

          foreach ($agent in $agentResults) {
            Write-Output "  $($agent.Name): $($agent.Result)"
            if ($agent.Result -in @('failure', 'cancelled')) {
              $failedAgents += $agent.Name
              Write-Output "::error::$($agent.Name) agent failed with result: $($agent.Result)"
            }
          }

          if ($failedAgents.Count -gt 0) {
            "has_failures=true" >> $env:GITHUB_OUTPUT
            Write-Output "::warning::Failed agents: $($failedAgents -join ', ')"
            Write-Output "::warning::Check individual job logs for details"
          } else {
            "has_failures=false" >> $env:GITHUB_OUTPUT
          }

      - name: Checkout repository
        uses: actions/checkout@34e114876b0b11c390a56381ad16ebd13914f8d5

      - name: Download all review artifacts
        uses: actions/download-artifact@d3f86a106a0bac45b974a628896c90dbdf5c8093
        with:
          pattern: review-*
          path: ai-review-results
          merge-multiple: true

      - name: Validate artifact download
        shell: pwsh -NoProfile -Command "& '{0}'"
        run: |
          $ErrorActionPreference = 'Stop'
          Write-Output "Checking for required verdict files..."

          $agents = @('security', 'qa', 'analyst', 'architect', 'devops', 'roadmap')
          $missingFiles = @()

          foreach ($agent in $agents) {
            $verdictFile = "ai-review-results/$agent-verdict.txt"
            if (-not (Test-Path $verdictFile)) {
              $missingFiles += "$agent-verdict.txt"
            }
          }

          if ($missingFiles.Count -gt 0) {
            Write-Output "::error::Artifact download incomplete - missing files:"
            foreach ($file in $missingFiles) {
              Write-Output "::error::  - $file"
            }
            Write-Output "::error::This indicates agent review jobs failed or artifacts were not uploaded"
            exit 1
          }

          Write-Output "‚úì All required verdict files present"

      - name: Load review results
        id: load-results
        shell: pwsh -NoProfile -Command "& '{0}'"
        run: |
          $ErrorActionPreference = 'Stop'
          $agents = @('security', 'qa', 'analyst', 'architect', 'devops', 'roadmap')

          Write-Output "Loaded verdicts:"

          foreach ($agent in $agents) {
            $verdictFile = "ai-review-results/$agent-verdict.txt"
            $infraFile = "ai-review-results/$agent-infrastructure-failure.txt"

            # Read verdict (file existence already validated)
            $verdict = (Get-Content $verdictFile -Raw).Trim()

            # Read infrastructure flag
            $infra = if (Test-Path $infraFile) { (Get-Content $infraFile -Raw).Trim() } else { "false" }

            Write-Output "  ${agent}: $verdict (infra: $infra)"

            # Export to GITHUB_OUTPUT
            "${agent}_verdict=$verdict" >> $env:GITHUB_OUTPUT
            "${agent}_infra=$infra" >> $env:GITHUB_OUTPUT
          }

      - name: Aggregate Verdicts
        id: aggregate
        env:
          SECURITY_VERDICT: ${{ steps.load-results.outputs.security_verdict }}
          QA_VERDICT: ${{ steps.load-results.outputs.qa_verdict }}
          ANALYST_VERDICT: ${{ steps.load-results.outputs.analyst_verdict }}
          ARCHITECT_VERDICT: ${{ steps.load-results.outputs.architect_verdict }}
          DEVOPS_VERDICT: ${{ steps.load-results.outputs.devops_verdict }}
          ROADMAP_VERDICT: ${{ steps.load-results.outputs.roadmap_verdict }}
          SECURITY_INFRA: ${{ steps.load-results.outputs.security_infra }}
          QA_INFRA: ${{ steps.load-results.outputs.qa_infra }}
          ANALYST_INFRA: ${{ steps.load-results.outputs.analyst_infra }}
          ARCHITECT_INFRA: ${{ steps.load-results.outputs.architect_infra }}
          DEVOPS_INFRA: ${{ steps.load-results.outputs.devops_infra }}
          ROADMAP_INFRA: ${{ steps.load-results.outputs.roadmap_infra }}
        run: python3 .github/scripts/aggregate_quality_verdicts.py

      - name: Generate Report
        id: report
        env:
          RUN_ID: ${{ github.run_id }}
          SERVER_URL: ${{ github.server_url }}
          REPOSITORY: ${{ github.repository }}
          EVENT_NAME: ${{ github.event_name }}
          REF_NAME: ${{ github.ref_name }}
          SHA: ${{ github.sha }}
          FINAL_VERDICT: ${{ steps.aggregate.outputs.final_verdict }}
          SECURITY_VERDICT: ${{ steps.aggregate.outputs.security_verdict }}
          QA_VERDICT: ${{ steps.aggregate.outputs.qa_verdict }}
          ANALYST_VERDICT: ${{ steps.aggregate.outputs.analyst_verdict }}
          ARCHITECT_VERDICT: ${{ steps.aggregate.outputs.architect_verdict }}
          DEVOPS_VERDICT: ${{ steps.aggregate.outputs.devops_verdict }}
          ROADMAP_VERDICT: ${{ steps.aggregate.outputs.roadmap_verdict }}
          SECURITY_CATEGORY: ${{ steps.aggregate.outputs.security_category }}
          QA_CATEGORY: ${{ steps.aggregate.outputs.qa_category }}
          ANALYST_CATEGORY: ${{ steps.aggregate.outputs.analyst_category }}
          ARCHITECT_CATEGORY: ${{ steps.aggregate.outputs.architect_category }}
          DEVOPS_CATEGORY: ${{ steps.aggregate.outputs.devops_category }}
          ROADMAP_CATEGORY: ${{ steps.aggregate.outputs.roadmap_category }}
        run: python3 .github/scripts/generate_quality_report.py

      - name: Check for infrastructure failures and add label
        continue-on-error: true
        shell: pwsh -NoProfile -Command "& '{0}'"
        env:
          GH_TOKEN: ${{ github.token }}
          PR_NUMBER: ${{ env.PR_NUMBER }}
        run: |
          $ErrorActionPreference = 'Stop'

          # Check if any agent had infrastructure failure (Issue #328: Add infrastructure-failure label when retry logic exhausted)
          $infrastructureFailureDetected = $false
          $retryAttempts = @()

          $agents = @('security', 'qa', 'analyst', 'architect', 'devops', 'roadmap')

          foreach ($agent in $agents) {
            $infraFile = "ai-review-results/$agent-infrastructure-failure.txt"
            $retryFile = "ai-review-results/$agent-retry-count.txt"

            if (Test-Path $infraFile) {
              $infraFlag = (Get-Content $infraFile -Raw).Trim()
              if ($infraFlag -eq "true") {
                $infrastructureFailureDetected = $true
                $retryCount = 0
                if (Test-Path $retryFile) {
                  $retryCount = [int](Get-Content $retryFile -Raw).Trim()
                }
                $retryAttempts += "${agent}: $retryCount retries"
                Write-Output "::notice::Infrastructure failure detected for $agent agent (retries: $retryCount)"
              }
            }
          }

          if ($infrastructureFailureDetected) {
            Write-Output "::warning::Infrastructure failures detected - adding infrastructure-failure label"
            Write-Output "Retry attempts: $($retryAttempts -join ', ')"

            try {
              # Verify gh CLI is authenticated
              $authStatus = gh auth status 2>&1
              if ($LASTEXITCODE -ne 0) {
                Write-Output "::warning::gh CLI authentication failed, cannot add label"
                exit 0
              }

              # Add label (best-effort, should not block aggregation)
              gh pr edit $env:PR_NUMBER --repo $env:GITHUB_REPOSITORY --add-label "infrastructure-failure"

              if ($LASTEXITCODE -ne 0) {
                Write-Output "::warning::Failed to add infrastructure-failure label (exit code: $LASTEXITCODE)"
              } else {
                Write-Output "Successfully added infrastructure-failure label"
              }
            }
            catch {
              Write-Output "::warning::Failed to add label: $_"
            }
          } else {
            Write-Output "No infrastructure failures detected"
          }

      - name: Post PR Comment
        env:
          GH_TOKEN: ${{ github.token }}
          PR_NUMBER: ${{ env.PR_NUMBER }}
          REPORT_FILE: ${{ steps.report.outputs.report_file }}
        run: |
          if [ -z "$PR_NUMBER" ]; then
            echo "::error::PR_NUMBER environment variable is missing"
            exit 1
          fi
          if [ -z "$REPORT_FILE" ]; then
            echo "::error::REPORT_FILE environment variable is missing"
            exit 1
          fi
          if [ ! -f "$REPORT_FILE" ]; then
            echo "::error::Report file not found: $REPORT_FILE"
            exit 1
          fi
          python3 .github/scripts/post_issue_comment.py \
            --issue "$PR_NUMBER" \
            --body-file "$REPORT_FILE" \
            --marker "AI-PR-QUALITY-GATE" \
            --update-if-exists

      - name: Set Job Summary
        shell: pwsh -NoProfile -Command "& '{0}'"
        env:
          REPORT_FILE: ${{ steps.report.outputs.report_file }}
        run: |
          $ErrorActionPreference = 'Stop'
          Get-Content $env:REPORT_FILE | Add-Content $env:GITHUB_STEP_SUMMARY

      - name: Check for Critical Failures
        shell: pwsh -NoProfile -Command "& '{0}'"
        env:
          FINAL_VERDICT: ${{ steps.aggregate.outputs.final_verdict }}
          SECURITY_VERDICT: ${{ steps.aggregate.outputs.security_verdict }}
          QA_VERDICT: ${{ steps.aggregate.outputs.qa_verdict }}
          ANALYST_VERDICT: ${{ steps.aggregate.outputs.analyst_verdict }}
          ARCHITECT_VERDICT: ${{ steps.aggregate.outputs.architect_verdict }}
          DEVOPS_VERDICT: ${{ steps.aggregate.outputs.devops_verdict }}
          ROADMAP_VERDICT: ${{ steps.aggregate.outputs.roadmap_verdict }}
        run: |
          $ErrorActionPreference = 'Stop'
          $finalVerdict = $env:FINAL_VERDICT

          # Use ordered array for deterministic output (PowerShell dictionary Keys property is unordered, which would cause agent order to vary across runs, complicating comparisons)
          $agentVerdicts = @(
            @{ Name = 'üîí Security'; Verdict = $env:SECURITY_VERDICT }
            @{ Name = 'üß™ QA'; Verdict = $env:QA_VERDICT }
            @{ Name = 'üìä Analyst'; Verdict = $env:ANALYST_VERDICT }
            @{ Name = 'üìê Architect'; Verdict = $env:ARCHITECT_VERDICT }
            @{ Name = '‚öôÔ∏è DevOps'; Verdict = $env:DEVOPS_VERDICT }
            @{ Name = 'üó∫Ô∏è Roadmap'; Verdict = $env:ROADMAP_VERDICT }
          )

          # Identify agents with blocking verdicts
          # NEEDS_REVIEW added in Issue #470 fix - indicates AI couldn't produce explicit verdict (occurs when AI response doesn't match expected verdict format, treating ambiguity as blocking)
          $blockingVerdicts = @('CRITICAL_FAIL', 'REJECTED', 'FAIL', 'NEEDS_REVIEW')
          $failedAgents = @()
          $missingVerdicts = @()

          foreach ($entry in $agentVerdicts) {
            if ([string]::IsNullOrWhiteSpace($entry.Verdict)) {
              $missingVerdicts += $entry.Name
              Write-Output "::error::$($entry.Name): No verdict received from aggregate step"
              continue
            }
            if ($entry.Verdict -in $blockingVerdicts) {
              $failedAgents += "$($entry.Name): $($entry.Verdict)"
              Write-Output "::error::$($entry.Name): $($entry.Verdict)"
            }
          }

          if ($missingVerdicts.Count -gt 0) {
            Write-Output ""
            Write-Output "‚ùå Quality gate failed - missing verdicts"
            Write-Output ""
            Write-Output "Agents with missing verdicts:"
            foreach ($missing in $missingVerdicts) {
              Write-Output "  - $missing"
            }
            Write-Output ""
            Write-Output "This indicates agent review jobs failed or artifacts are incomplete"
            exit 1
          }

          if ($finalVerdict -in $blockingVerdicts) {
            Write-Output ""
            Write-Output "‚ùå AI Quality Gate FAILED"
            Write-Output ""
            Write-Output "Agents with blocking verdicts:"
            foreach ($failed in $failedAgents) {
              Write-Output "  - $failed"
            }
            Write-Output ""
            Write-Output "Click on individual agent jobs above to see detailed findings."
            exit 1
          }

          Write-Output "‚úÖ AI Quality Gate passed with verdict: $finalVerdict"
